{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayers:\n",
    "  def __init__(self,kernel_num,kernel_size,stride = 1,bias = 1):\n",
    "\n",
    "    self.no_of_kernels = kernel_num\n",
    "    self.kernel_size = kernel_size\n",
    "    self.stride = stride\n",
    "    self.bias = bias\n",
    "    self.kernels = np.random.randn(kernel_num,kernel_size,kernel_size) / kernel_size**2\n",
    "  def partition_generator(self,input_img):\n",
    "    img_h,img_w = input_img.shape\n",
    "    self.img = input_img\n",
    "    for h in range(0,img_h-self.kernel_size+1,self.stride):\n",
    "      for w in range(0,img_w - self.kernel_size+1,self.stride):\n",
    "        slice = input_img[h:(h+self.kernel_size),w:(w+self.kernel_size)]\n",
    "        yield slice,h,w\n",
    "  def forward_prop(self,img):\n",
    "    img_h,img_w = img.shape\n",
    "    conv_output = np.zeros((img_h-self.kernel_size+1,img_w-self.kernel_size+1,self.no_of_kernels))\n",
    "    for sec,h,w in self.partition_generator(img):\n",
    "      conv_output[h,w] = np.sum(sec*self.kernels,axis = (1,2))\n",
    "    return conv_output\n",
    "  def backward_prop(self,dL_dZ,learning_rate):\n",
    "    dL_dk = np.zeros(self.kernels.shape)\n",
    "    for sec,h,w in self.partition_generator(self.img):\n",
    "      for f in range(self.no_of_kernels):\n",
    "        dL_dk += sec*dL_dZ[h,w,f]\n",
    "    self.kernels -= learning_rate*dL_dk\n",
    "    return dL_dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "  def __init__(self,pool_size,stride=2):\n",
    "    self.pool_size = pool_size\n",
    "    self.stride = stride\n",
    "\n",
    "  def partition_generator(self,img):\n",
    "    img_h,img_w = img.shape[0]//self.pool_size,img.shape[1]//self.pool_size\n",
    "    self.img = img\n",
    "    for h in range(0,img_h,self.stride):\n",
    "      for w in range(0,img_w,self.stride):\n",
    "        slice = img[(h*self.pool_size):(h*self.pool_size+self.pool_size),(w*self.pool_size):(w*self.pool_size+self.pool_size)]\n",
    "        yield slice,h,w\n",
    "  def forward_prop(self,img):\n",
    "    h,w,num_kernels = img.shape\n",
    "    max_pooled_out = np.zeros((h//self.pool_size,w//self.pool_size,num_kernels))\n",
    "    for slice,h,w in self.partition_generator(img):\n",
    "      max_pooled_out[h,w] = np.amax(slice,axis = (0,1))\n",
    "    return max_pooled_out\n",
    "\n",
    "  def backward_prop(self,dL_dZ):\n",
    "    dL_dk = np.zeros(self.img.shape)\n",
    "    for patch,h,w in self.partition_generator(self.img):\n",
    "      h0, w0, num_kernels = patch.shape\n",
    "      max_val = np.amax(patch, axis=(0,1))\n",
    "      for idx_h in range(h0):\n",
    "        for idx_w in range(w0):\n",
    "          for idx_k in range(num_kernels):\n",
    "            if patch[idx_h,idx_w,idx_k] == max_val[idx_k]:\n",
    "              dL_dk[h*self.pool_size+idx_h, w*self.pool_size+idx_w, idx_k] = dL_dZ[h,w,idx_k]\n",
    "    return dL_dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyCon_SoftmaxLayer:\n",
    "    def __init__(self, input_units, output_units):\n",
    "        self.weight = np.random.randn(input_units, output_units)/input_units\n",
    "        self.bias = np.zeros(output_units)\n",
    "        self.output = None\n",
    "\n",
    "    def _dense_layer(self,image):\n",
    "        self.original_shape = image.shape\n",
    "        image_flattened = image.flatten()\n",
    "        self.flattened_input = image_flattened\n",
    "        dense_output = np.dot(image_flattened, self.weight) + self.bias\n",
    "        self.output = dense_output\n",
    "        return dense_output\n",
    "\n",
    "    def _softmax_out(self,dense_out):\n",
    "        softmax_output = np.exp(dense_out) / np.sum(np.exp(dense_out), axis=0)\n",
    "        return softmax_output\n",
    "\n",
    "    def forward_prop(self, image):\n",
    "        dense_out = self._dense_layer(image)\n",
    "        softmax_output = self._softmax_out(dense_out)\n",
    "        return softmax_output\n",
    "\n",
    "    def backward_prop(self, dL_dz, lr):\n",
    "        for i, gradient in enumerate(dL_dz):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "            transformation_eq = np.exp(self.output)\n",
    "            S_total = np.sum(transformation_eq)\n",
    "\n",
    "            dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
    "            dY_dZ[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total**2)\n",
    "\n",
    "            dZ_dw = self.flattened_input\n",
    "            dZ_db = 1\n",
    "            dZ_dX = self.weight\n",
    "\n",
    "            dE_dZ = gradient * dY_dZ\n",
    "            dE_dw = dZ_dw[np.newaxis].T @ dE_dZ[np.newaxis]\n",
    "            dE_db = dE_dZ * dZ_db\n",
    "            dE_dX = dZ_dX @ dE_dZ\n",
    "\n",
    "            self.weight -= lr*dE_dw\n",
    "            self.bias -= lr*dE_db\n",
    "\n",
    "            return dE_dX.reshape(self.original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Pred_Model:\n",
    "    def __init__(self,epochs = 10,img = None,labels = None,layers = None,isMnist = True,train = True):\n",
    "      self.isMnist = isMnist\n",
    "      self.img = img\n",
    "      self.labels = labels\n",
    "      self.layers = layers\n",
    "      self.epochs = epochs\n",
    "      \n",
    "      if self.isMnist and train:\n",
    "        self._Mnist_Exec()\n",
    "      elif train:\n",
    "        self._Normal_Exec()\n",
    "\n",
    "    def _Mnist_Exec(self):\n",
    "      (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "      X_train = X_train[:5000]\n",
    "      y_train = y_train[:5000]\n",
    "\n",
    "      layers = [\n",
    "          ConvolutionLayers(16,3),\n",
    "          MaxPool(2),\n",
    "          FullyCon_SoftmaxLayer(13*13*16, 10)\n",
    "          ]\n",
    "\n",
    "      for epoch in range(self.epochs):\n",
    "          print('Epoch {} ->'.format(epoch+1))\n",
    "\n",
    "          permutation = np.random.permutation(len(X_train))\n",
    "          X_train = X_train[permutation]\n",
    "          y_train = y_train[permutation]\n",
    "\n",
    "          loss = 0\n",
    "          accuracy = 0\n",
    "          for i, (image, label) in enumerate(zip(X_train, y_train)):\n",
    "            if i % 100 == 0:\n",
    "              print(\"Step {}. For the last 100 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
    "              loss = 0\n",
    "              accuracy = 0\n",
    "            loss_1, accuracy_1 = self.train(image, label, layers)\n",
    "            loss += loss_1\n",
    "            accuracy += accuracy_1\n",
    "      return layers\n",
    "    \n",
    "    def _Normal_Exec(self): \n",
    "      for i in range(self.epochs):\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        for i, (image, label) in enumerate(zip(self.imgs, self.labels)):\n",
    "            if i % 100 == 0:\n",
    "                print(\"Step {}. For the last 100 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
    "                loss = 0\n",
    "                accuracy = 0\n",
    "            loss_1, accuracy_1 = self.train(image, label, self.layers)\n",
    "            loss += loss_1\n",
    "            accuracy += accuracy_1\n",
    "        print(f\"Epoch {i}--> loss: {loss}  accuracy: {accuracy}\") \n",
    "      return self.layers\n",
    "\n",
    "    def _forward(self,img,label,layers):\n",
    "        output = img/255.\n",
    "        for layer in layers:\n",
    "          output = layer.forward_prop(output)\n",
    "        loss = -np.log(output[label])\n",
    "        acc = 1 if np.argmax(output) == label else 0\n",
    "        return output,loss,acc\n",
    "    \n",
    "    def _backprop(self,gradients,layers,lr = 0.01):\n",
    "        grad = gradients\n",
    "        for layer in layers[::-1]:\n",
    "          if type(layer) in [ConvolutionLayers,FullyCon_SoftmaxLayer]:\n",
    "            grad = layer.backward_prop(grad,lr)\n",
    "          else:\n",
    "            grad = layer.backward_prop(grad)\n",
    "        return grad\n",
    "    def train(self,img,label,layers,lr=0.05):\n",
    "        output,loss,acc = self._forward(img,label,layers)\n",
    "        gradient = np.zeros(10)\n",
    "        gradient[label] = -1/output[label]\n",
    "        grad = self._backprop(gradient,layers,lr)\n",
    "        return loss,acc\n",
    "\n",
    "    def predict(self,img,layers):\n",
    "        output = img/255.\n",
    "        for layer in layers:\n",
    "          output = layer.forward_prop(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss 2.6287586402564234, accuracy 22\n",
      "Step 201. For the last 100 steps: average loss 1.9588065311567378, accuracy 42\n",
      "Step 301. For the last 100 steps: average loss 2.412216035322477, accuracy 37\n",
      "Step 401. For the last 100 steps: average loss 4.229739562509493, accuracy 33\n",
      "Step 501. For the last 100 steps: average loss 22.616304622931967, accuracy 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18643/3490523291.py:31: RuntimeWarning: overflow encountered in scalar power\n",
      "  dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
      "/tmp/ipykernel_18643/3490523291.py:32: RuntimeWarning: overflow encountered in scalar power\n",
      "  dY_dZ[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total**2)\n",
      "/tmp/ipykernel_18643/3490523291.py:31: RuntimeWarning: overflow encountered in multiply\n",
      "  dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
      "/tmp/ipykernel_18643/3490523291.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
      "/tmp/ipykernel_18643/3490523291.py:32: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  dY_dZ[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total**2)\n",
      "/tmp/ipykernel_18643/3490523291.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dY_dZ[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 601. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Epoch 2 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Epoch 3 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 19\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 19\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Epoch 4 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 18\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 3\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Epoch 5 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Epoch 6 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 18\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 3\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Epoch 7 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 3\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 20\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Epoch 8 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Epoch 9 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 3\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Epoch 10 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Epoch 11 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 3\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Epoch 12 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 2\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 15\n",
      "Epoch 13 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 18\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 3\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 4\n",
      "Epoch 14 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 2\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 15\n",
      "Epoch 15 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 13\n",
      "Epoch 16 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Epoch 17 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 1\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 18\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 16\n",
      "Epoch 18 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 17\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Epoch 19 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 2\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 2\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Epoch 20 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Epoch 21 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 17\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 3\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Epoch 22 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 3\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Epoch 23 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Epoch 24 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 3\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Epoch 25 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 17\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Epoch 26 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 5\n",
      "Epoch 27 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Epoch 28 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 4\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Epoch 29 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 3\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Epoch 30 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 3\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 17\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 4\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# Model = Train_Pred_Model(30,train = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[0.09992258 0.09981661 0.09997726 0.09999994 0.09994578 0.09989672\n",
      " 0.10009112 0.09989858 0.10018644 0.10026496]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(y_test[9])\n",
    "img = X_test[9]\n",
    "layers = [\n",
    "    ConvolutionLayers(16,3),\n",
    "    MaxPool(2),\n",
    "    FullyCon_SoftmaxLayer(13*13*16, 10)\n",
    "    ]\n",
    "pre = Train_Pred_Model(train = False)\n",
    "pred = pre.predict(img,layers)\n",
    "print(pred)\n",
    "print(np.argmax(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
